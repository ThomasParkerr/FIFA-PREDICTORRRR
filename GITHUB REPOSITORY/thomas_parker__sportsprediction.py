# -*- coding: utf-8 -*-
"""THOMAS_PARKER._SportsPrediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rJDijEcCkSveGpfqoAB9fGbIQiMh8u7J
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
import pickle
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Load the datasets
Legacy_Players = pd.read_csv('male_players (legacy).csv', low_memory=False)
Next_gen = pd.read_csv('players_22.csv', low_memory=False)

Legacy_Players.head()

Next_gen.head()

# Inspect the shape of the datasets
print("Training data shape:", Legacy_Players.shape)
print("Test data shape:", Next_gen.shape)

# Inspect missing values in the datasets
print("Missing values in training data:\n", Legacy_Players.isnull().sum())
print("Missing values in test data:\n", Next_gen.isnull().sum())

missing_percent = Legacy_Players.isnull().sum() / len(Legacy_Players)

# Display the percentage of missing data
print(missing_percent)

# Drop columns with more than 30% missing values
threshold = 0.30
columns_to_drop = missing_percent[missing_percent > threshold].index

Legacy_Players = Legacy_Players.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
Legacy_Players.head()

Legacy_Players.drop(columns = ['player_id', 'player_url', 'fifa_version', 'fifa_update', 'fifa_update_date', 'player_face_url', 'short_name', 'long_name'], inplace = True)

Legacy_Players.head()

Quantitative = Legacy_Players.select_dtypes(include=[np.number])
corr_matrix = Quantitative.corr()
columns_to_drop = [column for column in Quantitative.columns if abs(corr_matrix['overall'][column]) < 0.4]
Quantitative.drop(columns=columns_to_drop, axis=1, inplace=True)
corr_matrix['overall'].sort_values(ascending=False)
Quantitative.columns

important_cols = ['body_type', 'preferred_foot']
Categorical = Legacy_Players.select_dtypes(include = ['object'])
Categorical.drop(columns = [column for column in Categorical.columns if column not in important_cols], inplace =True)
Categorical.columns

QuantandCat = pd.concat([Categorical, Quantitative], axis = 1)
QuantandCat.columns

# common relevant columns present in both datasets
relevant_columns = [col for col in Quantitative if col in Legacy_Players.columns and col in Next_gen.columns]

# Extracting features and target variable from the training data
X = Legacy_Players[relevant_columns]
Y = Legacy_Players['overall']



# Handle categorical features using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Fill missing values with the mean for numerical features
X.fillna(X.mean(), inplace=True)

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the training data into training and validation sets
X, X_val, Y, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize the ML models

models = {
    'Linear Regression': LinearRegression(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Random Forest': RandomForestRegressor()
}

linearmod=LinearRegression()
DeciderTree = DecisionTreeRegressor()
GradBoost = GradientBoostingRegressor()
RandFor = RandomForestRegressor()

results = {}
for name, model in models.items():
    scores = cross_val_score(model, X, Y, cv=5, scoring='neg_mean_squared_error')
    results[name] = np.sqrt(-scores.mean())

print("Model Evaluation Results: ", results)

with open('linear_model.pkl', 'wb') as file:
    pickle.dump(linearmod, file)  # Save the linear regression model

with open('GradBoost.pkl', 'wb') as file:
    pickle.dump(GradBoost, file)  # Save the regression model

with open('RandFor.pkl', 'wb') as file:
    pickle.dump(RandFor, file)  # Save the regression model

with open('DeciderTree.pkl', 'wb') as file:
    pickle.dump(DeciderTree, file)  # Save the regression model

params = {
    'n_estimators': [100,200,300],
    'max_depth': [5,7,9]
}

rando_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=params,
    n_iter=9,  # Reduce number of iterations
    cv=2,  # Reduce number of cross-validation folds
    n_jobs=-1,  # Use all available cores
    random_state=42
)

rando_search.fit(X, Y)

best_model = rando_search.best_estimator_
best_model

with open('best_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)  # Save the regression model

# Train the best model on the full training data and evaluate on the validation set
best_model.fit(X, Y)
y_pred = best_model.predict(X_val)
print("Validation RMSE:", np.sqrt(mean_squared_error(y_val, y_pred)))
best_model

# Prepare the test data in a similar way
Gen_test = Next_gen[relevant_columns]
Gen_test = pd.get_dummies(Gen_test, drop_first=True)
Gen_test.fillna(Gen_test.mean(), inplace=True)
Gen_test = scaler.transform(Gen_test)

# Predict the overall ratings for the test data
Next_gen['predicted_overall'] = best_model.predict(Gen_test)

# Save the predictions
Next_gen.to_csv('predicted_players_22.csv', index=False)
print("Predictions saved to predicted_players_22.csv")

filename = 'predicted_players_22.pkl'
pickle.dump(filename, open(filename, 'wb'))
saved_file = pickle.load(open(filename, 'rb'))

# Save the model
loaded_model = 'best_model.pkl'
pickle.dump(best_model, open(loaded_model, 'wb'))
loaded_model = pickle.load(open(loaded_model, 'rb'))